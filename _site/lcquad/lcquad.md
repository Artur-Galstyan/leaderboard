# Large-scale Complex Question Answering

Question answering is the task of answering a question.

### Table of contents

- [LC-QuAD 1.0](#lc-quad-v1)
- [LC-QuAD 2.0](#lc-quad-v2)
  
  
## LC-QuAD v1

The [Largescale Complex Question Answering Dataset 1.0 (LC-QuAD 1.0)](http://lc-quad.sda.tech/static/ISWC2017_paper_152.pdf) LC-QuAD is a Question Answering dataset with 5000 pairs of question and its corresponding SPARQL query. The target knowledge base is DBpedia, specifically, the April, 2016 version. Please see our paper for details about the dataset creation process and framework.

### Leaderboard

| Model           | F1 | EM |  Paper |
| ------------- | :-----:| :-----:| --- |
| BERT (Efimov et al., 2019) | 84.8 | 66.6 | [SberQuAD - Russian Reading Comprehension Dataset: Description and Analysis](https://arxiv.org/abs/1912.09723) |
| DocQA (Efimov et al., 2019) | 79.5 | 59.6 | [SberQuAD - Russian Reading Comprehension Dataset: Description and Analysis](https://arxiv.org/abs/1912.09723) |


## LC-QuAD v2

The [Largescale Complex Question Answering Dataset 2.0 (LC-QuAD 2.0)](http://jens-lehmann.org/files/2019/iswc_lcquad2.pdf) is a Large Question Answering dataset with 30,000 pairs of question 
and its corresponding SPARQL query. The target knowledge base is Wikidata and DBpedia, specifically the 2018 version. Please see our paper for details about the dataset creation process 
and framework.

### Leaderboard

| Model           | F1 | EM |  Paper |
| ------------- | :-----:| :-----:| --- |
| BERT (Efimov et al., 2019) | 84.8 | 66.6 | [SberQuAD - Russian Reading Comprehension Dataset: Description and Analysis](https://arxiv.org/abs/1912.09723) |
| DocQA (Efimov et al., 2019) | 79.5 | 59.6 | [SberQuAD - Russian Reading Comprehension Dataset: Description and Analysis](https://arxiv.org/abs/1912.09723) |

[Go back to the README](../README.md)